{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from operator import itemgetter\n",
    "import nest_asyncio\n",
    "\n",
    "# LANGCHAIN\n",
    "from langchain import hub\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.tracers.log_stream import LogStreamCallbackHandler\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain with sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing\n",
    "bs_strainer = bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits, embedding=OllamaEmbeddings(model=\"llama2:13b\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval and Generation\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = Ollama(model=\"llama2:13b\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming final outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is task decomposition?'}\n",
      "{'context': [Document(page_content='Fig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='LSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='HNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='They did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})]}\n",
      "{'answer': 'Task'}\n",
      "{'answer': ' decomposition'}\n",
      "{'answer': ' refers'}\n",
      "{'answer': ' to'}\n",
      "{'answer': ' the'}\n",
      "{'answer': ' process'}\n",
      "{'answer': ' of'}\n",
      "{'answer': ' breaking'}\n",
      "{'answer': ' down'}\n",
      "{'answer': ' a'}\n",
      "{'answer': ' complex'}\n",
      "{'answer': ' task'}\n",
      "{'answer': ' into'}\n",
      "{'answer': ' smaller'}\n",
      "{'answer': ','}\n",
      "{'answer': ' more'}\n",
      "{'answer': ' manage'}\n",
      "{'answer': 'able'}\n",
      "{'answer': ' sub'}\n",
      "{'answer': '-'}\n",
      "{'answer': 'tasks'}\n",
      "{'answer': '.'}\n",
      "{'answer': ' This'}\n",
      "{'answer': ' is'}\n",
      "{'answer': ' done'}\n",
      "{'answer': ' by'}\n",
      "{'answer': ' the'}\n",
      "{'answer': ' language'}\n",
      "{'answer': ' model'}\n",
      "{'answer': ' ('}\n",
      "{'answer': 'LL'}\n",
      "{'answer': 'M'}\n",
      "{'answer': ')'}\n",
      "{'answer': ' in'}\n",
      "{'answer': ' the'}\n",
      "{'answer': ' H'}\n",
      "{'answer': 'ug'}\n",
      "{'answer': 'ging'}\n",
      "{'answer': 'G'}\n",
      "{'answer': 'PT'}\n",
      "{'answer': ' system'}\n",
      "{'answer': ','}\n",
      "{'answer': ' which'}\n",
      "{'answer': ' par'}\n",
      "{'answer': 's'}\n",
      "{'answer': 'es'}\n",
      "{'answer': ' user'}\n",
      "{'answer': ' requests'}\n",
      "{'answer': ' into'}\n",
      "{'answer': ' multiple'}\n",
      "{'answer': ' tasks'}\n",
      "{'answer': ' and'}\n",
      "{'answer': ' their'}\n",
      "{'answer': ' associated'}\n",
      "{'answer': ' attributes'}\n",
      "{'answer': ','}\n",
      "{'answer': ' such'}\n",
      "{'answer': ' as'}\n",
      "{'answer': ' task'}\n",
      "{'answer': ' type'}\n",
      "{'answer': ','}\n",
      "{'answer': ' ID'}\n",
      "{'answer': ','}\n",
      "{'answer': ' dependencies'}\n",
      "{'answer': ','}\n",
      "{'answer': ' and'}\n",
      "{'answer': ' arguments'}\n",
      "{'answer': '.'}\n",
      "{'answer': ' The'}\n",
      "{'answer': ' system'}\n",
      "{'answer': ' uses'}\n",
      "{'answer': ' few'}\n",
      "{'answer': '-'}\n",
      "{'answer': 'shot'}\n",
      "{'answer': ' examples'}\n",
      "{'answer': ' to'}\n",
      "{'answer': ' guide'}\n",
      "{'answer': ' the'}\n",
      "{'answer': ' L'}\n",
      "{'answer': 'LM'}\n",
      "{'answer': ' in'}\n",
      "{'answer': ' performing'}\n",
      "{'answer': ' task'}\n",
      "{'answer': ' parsing'}\n",
      "{'answer': ' and'}\n",
      "{'answer': ' planning'}\n",
      "{'answer': '.'}\n",
      "{'answer': ''}\n"
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain_with_source.stream(\"What is task decomposition?\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "question: What is task decomposition?\n",
      "\n",
      "context: [Document(page_content='Fig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='LSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='HNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='They did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})]\n",
      "\n",
      "answer: Task decomposition is a process of breaking down complex tasks into smaller, more manageable sub-tasks. This is achieved through a system of four stages: task planning, instruction, LSH (Locality-Sensitive Hashing), and HNSW (Hierarchical Navigable Small World). The system uses few-shot examples to guide the process and ensure that similar input items are mapped to the same buckets with high probability. The goal of task decomposition is to make it easier to solve complex tasks by breaking them down into smaller, more manageable parts."
     ]
    }
   ],
   "source": [
    "output = {}\n",
    "curr_key = None\n",
    "for chunk in rag_chain_with_source.stream(\"What is task decomposition?\"):\n",
    "    for key in chunk:\n",
    "        if key not in output:\n",
    "            output[key] = chunk[key]\n",
    "        else:\n",
    "            output[key] += chunk[key]\n",
    "        \n",
    "        if key != curr_key:\n",
    "            print(f\"\\n\\n{key}: {chunk[key]}\", end=\"\", flush=True)\n",
    "        else:\n",
    "            print(chunk[key], end=\"\", flush=True)\n",
    "        curr_key = key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming intermediate steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "contextualize_q_chain = (\n",
    "    contextualize_q_prompt | llm | StrOutputParser()\n",
    ").with_config(tags=[\"contextualize_q_chain\"])\n",
    "\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "def contextualized_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return contextualize_q_chain\n",
    "    else:\n",
    "        return input[\"question\"]\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(context=contextualize_q_chain | retriever | format_docs)\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '',\n",
      "  'value': {'final_output': None,\n",
      "            'id': '59bbf000-f9ff-40b5-b516-45e9ec1b6050',\n",
      "            'logs': {},\n",
      "            'name': 'RunnableSequence',\n",
      "            'streamed_output': [],\n",
      "            'type': 'chain'}})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/RunnableSequence',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': 'e6558081-b646-485b-b98a-b6a90b87dbeb',\n",
      "            'metadata': {},\n",
      "            'name': 'RunnableSequence',\n",
      "            'start_time': '2024-04-17T01:46:42.387+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': ['seq:step:1', 'contextualize_q_chain'],\n",
      "            'type': 'chain'}})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatPromptTemplate',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': 'ffa8df56-ca22-4fb3-be8b-f909201d8a8f',\n",
      "            'metadata': {},\n",
      "            'name': 'ChatPromptTemplate',\n",
      "            'start_time': '2024-04-17T01:46:42.389+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': ['seq:step:1', 'contextualize_q_chain'],\n",
      "            'type': 'prompt'}})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/ChatPromptTemplate/final_output',\n",
      "  'value': ChatPromptValue(messages=[SystemMessage(content='Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.'), HumanMessage(content='What is task decomposition?'), HumanMessage(content='Task decomposition refers to the process of breaking down a complex task into smaller, more manageable sub-tasks or steps. This allows the AI assistant to focus on one task at a time and complete each step successfully before moving on to the next one. By decomposing tasks in this way, the AI assistant can better understand the specific requirements of the task and execute it more effectively.'), HumanMessage(content='What are common wawys of doint it?')])},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/ChatPromptTemplate/end_time',\n",
      "  'value': '2024-04-17T01:46:42.390+00:00'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/Ollama',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': '6c175349-8069-4955-8fa0-2fb78fd2dea0',\n",
      "            'metadata': {},\n",
      "            'name': 'Ollama',\n",
      "            'start_time': '2024-04-17T01:46:42.391+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': ['seq:step:2', 'contextualize_q_chain'],\n",
      "            'type': 'llm'}})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add', 'path': '/logs/Ollama/streamed_output_str/-', 'value': '\\n'},\n",
      " {'op': 'add', 'path': '/logs/Ollama/streamed_output/-', 'value': '\\n'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': 'f25b19f3-80da-4635-97f5-36ac11798141',\n",
      "            'metadata': {},\n",
      "            'name': 'StrOutputParser',\n",
      "            'start_time': '2024-04-17T01:46:43.739+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': ['seq:step:3', 'contextualize_q_chain'],\n",
      "            'type': 'parser'}})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': '\\n'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/RunnableSequence/streamed_output/-',\n",
      "  'value': '\\n'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add', 'path': '/logs/Ollama/streamed_output_str/-', 'value': 'Re'},\n",
      " {'op': 'add', 'path': '/logs/Ollama/streamed_output/-', 'value': 'Re'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'Re'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/RunnableSequence/streamed_output/-',\n",
      "  'value': 'Re'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add', 'path': '/logs/Ollama/streamed_output_str/-', 'value': 'form'},\n",
      " {'op': 'add', 'path': '/logs/Ollama/streamed_output/-', 'value': 'form'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'form'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/RunnableSequence/streamed_output/-',\n",
      "  'value': 'form'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add', 'path': '/logs/Ollama/streamed_output_str/-', 'value': 'ulated'},\n",
      " {'op': 'add', 'path': '/logs/Ollama/streamed_output/-', 'value': 'ulated'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': 'ulated'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/RunnableSequence/streamed_output/-',\n",
      "  'value': 'ulated'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/Ollama/streamed_output_str/-',\n",
      "  'value': ' question'},\n",
      " {'op': 'add', 'path': '/logs/Ollama/streamed_output/-', 'value': ' question'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/StrOutputParser/streamed_output/-',\n",
      "  'value': ' question'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/RunnableSequence/streamed_output/-',\n",
      "  'value': ' question'})\n",
      "\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "question = \"What is task decomposition?\"\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg])\n",
    "\n",
    "question = \"What are common wawys of doint it?\"\n",
    "ct = 0\n",
    "async for jsonpatch_op in rag_chain.astream_log(\n",
    "    {\"question\": question, \"chat_history\": chat_history},\n",
    "    include_tags=[\"contextualize_q_chain\"],\n",
    "):\n",
    "    print(jsonpatch_op)\n",
    "    print(\"\\n\" + \"-\" * 30 + \"\\n\")\n",
    "    ct += 1 \n",
    "    if ct > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '',\n",
      "  'value': {'final_output': None,\n",
      "            'id': '16b44684-bff1-45ae-9587-2101488004d7',\n",
      "            'logs': {},\n",
      "            'name': 'RunnableSequence',\n",
      "            'streamed_output': [],\n",
      "            'type': 'chain'}})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/Retriever',\n",
      "  'value': {'end_time': None,\n",
      "            'final_output': None,\n",
      "            'id': 'a8df5630-30f7-4740-9090-301ce73c16df',\n",
      "            'metadata': {},\n",
      "            'name': 'Retriever',\n",
      "            'start_time': '2024-04-17T01:48:31.634+00:00',\n",
      "            'streamed_output': [],\n",
      "            'streamed_output_str': [],\n",
      "            'tags': ['seq:step:2', 'Chroma', 'OllamaEmbeddings'],\n",
      "            'type': 'retriever'}})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'add',\n",
      "  'path': '/logs/Retriever/final_output',\n",
      "  'value': {'documents': [Document(page_content='This benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
      "                          Document(page_content='Memory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
      "                          Document(page_content='HNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
      "                          Document(page_content='They did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})]}},\n",
      " {'op': 'add',\n",
      "  'path': '/logs/Retriever/end_time',\n",
      "  'value': '2024-04-17T01:48:31.899+00:00'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace', 'path': '/final_output', 'value': ' '})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace', 'path': '/final_output', 'value': '  Sure'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace', 'path': '/final_output', 'value': '  Sure!'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace', 'path': '/final_output', 'value': '  Sure! Here'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace', 'path': '/final_output', 'value': '  Sure! Here is'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace', 'path': '/final_output', 'value': '  Sure! Here is my'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '  Sure! Here is my response'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '  Sure! Here is my response based'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '  Sure! Here is my response based on'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '  Sure! Here is my response based on the'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '  Sure! Here is my response based on the provided'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '  Sure! Here is my response based on the provided context'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '  Sure! Here is my response based on the provided context:'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '  Sure! Here is my response based on the provided context:\\n'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '  Sure! Here is my response based on the provided context:\\n\\n'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '  Sure! Here is my response based on the provided context:\\n'\n",
      "           '\\n'\n",
      "           'Task'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '  Sure! Here is my response based on the provided context:\\n'\n",
      "           '\\n'\n",
      "           'Task decomposition'})\n",
      "\n",
      "------------------------------\n",
      "\n",
      "RunLogPatch({'op': 'replace',\n",
      "  'path': '/final_output',\n",
      "  'value': '  Sure! Here is my response based on the provided context:\\n'\n",
      "           '\\n'\n",
      "           'Task decomposition refers'})\n",
      "\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ct = 0\n",
    "async for jsonpatch_op in rag_chain.astream_log(\n",
    "    {\"question\":question, \"chat_history\": chat_history},\n",
    "    include_names=[\"Retriever\"],\n",
    "    with_streamed_output_list=False\n",
    "):\n",
    "    print(jsonpatch_op)\n",
    "    print(\"\\n\" + \"-\" * 30 + \"\\n\")\n",
    "    ct += 1\n",
    "    if ct > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
