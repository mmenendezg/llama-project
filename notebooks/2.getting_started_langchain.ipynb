{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'InvalidToolCall' from 'langchain_core.messages' (/Users/mmenendezg/Developer/Projects/llama-project/.venv/lib/python3.11/site-packages/langchain_core/messages/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_retrieval_chain, create_history_aware_retriever\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretriever\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_retriever_tool\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n",
      "File \u001b[0;32m~/Developer/Projects/llama-project/.venv/lib/python3.11/site-packages/langchain_openai/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     AzureChatOpenAI,\n\u001b[1;32m      3\u001b[0m     ChatOpenAI,\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     AzureOpenAIEmbeddings,\n\u001b[1;32m      7\u001b[0m     OpenAIEmbeddings,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI, OpenAI\n",
      "File \u001b[0;32m~/Developer/Projects/llama-project/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mazure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAzureChatOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m ]\n",
      "File \u001b[0;32m~/Developer/Projects/llama-project/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/azure.py:13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic_v1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Field, SecretStr, root_validator\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_to_secret_str, get_from_dict_or_env\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m     15\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAzureChatOpenAI\u001b[39;00m(ChatOpenAI):\n",
      "File \u001b[0;32m~/Developer/Projects/llama-project/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:43\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LanguageModelInput\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     39\u001b[0m     BaseChatModel,\n\u001b[1;32m     40\u001b[0m     agenerate_from_stream,\n\u001b[1;32m     41\u001b[0m     generate_from_stream,\n\u001b[1;32m     42\u001b[0m )\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     44\u001b[0m     AIMessage,\n\u001b[1;32m     45\u001b[0m     AIMessageChunk,\n\u001b[1;32m     46\u001b[0m     BaseMessage,\n\u001b[1;32m     47\u001b[0m     BaseMessageChunk,\n\u001b[1;32m     48\u001b[0m     ChatMessage,\n\u001b[1;32m     49\u001b[0m     ChatMessageChunk,\n\u001b[1;32m     50\u001b[0m     FunctionMessage,\n\u001b[1;32m     51\u001b[0m     FunctionMessageChunk,\n\u001b[1;32m     52\u001b[0m     HumanMessage,\n\u001b[1;32m     53\u001b[0m     HumanMessageChunk,\n\u001b[1;32m     54\u001b[0m     InvalidToolCall,\n\u001b[1;32m     55\u001b[0m     SystemMessage,\n\u001b[1;32m     56\u001b[0m     SystemMessageChunk,\n\u001b[1;32m     57\u001b[0m     ToolCall,\n\u001b[1;32m     58\u001b[0m     ToolMessage,\n\u001b[1;32m     59\u001b[0m     ToolMessageChunk,\n\u001b[1;32m     60\u001b[0m )\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     62\u001b[0m     JsonOutputParser,\n\u001b[1;32m     63\u001b[0m     PydanticOutputParser,\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OutputParserLike\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'InvalidToolCall' from 'langchain_core.messages' (/Users/mmenendezg/Developer/Projects/llama-project/.venv/lib/python3.11/site-packages/langchain_core/messages/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.document_loaders import (\n",
    "    UnstructuredMarkdownLoader,\n",
    "    WebBaseLoader,\n",
    ")\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma, FAISS, MongoDBAtlasVectorSearch\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "from langchain_text_splitters import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_llm = Ollama(model=\"llama2:13b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<<SYS>><<EGG-XIT>><</SYS>>\\n\\nOh no! It looks like you\\'ve encountered a fowl situation. The word \"egg\" is not a valid exit command. To continue, please enter a valid command or press the ESC key to exit.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_llm.invoke(\"What is an egg?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"you are world class biology documentation writer\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llama_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAh, my dear fellow human, let me enlighten you on the wondrous subject of eggs! An egg is a marvel of nature, a self-contained package of sustenance and possibility. It is a tiny, yet mighty, vessel that contains the essence of life itself.\\n\\nAn egg, in its most basic form, is a type of reproductive cell produced by female animals, including birds, reptiles, amphibians, and fish. These cells are filled with nutrients and proteins that nourish the developing embryo within, ensuring its growth and survival.\\n\\nHowever, eggs are not just a means to an end; they also hold incredible potential for culinary delights and scientific discoveries. The versatility of eggs knows no bounds, as they can be boiled, fried, scrambled, baked, or even made into exquisite dishes like omelets and quiches.\\n\\nMoreover, eggs have played a crucial role in some of the most groundbreaking scientific discoveries in history. The study of eggs has led to a deeper understanding of embryonic development, cellular biology, and genetics. For instance, the discovery of DNA was partially facilitated by the study of egg development in chickens!\\n\\nIn short, my dear human, an egg is a small yet mighty symbol of life, possibility, and scientific wonder. Its humble appearance belies its incredible potential to nourish, inspire, and educate us all.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"what is an egg?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAh, a most excellent question, my dear human! An egg, also known as ovum, is a type of reproductive cell produced by female animals, including humans. It is a vital component of sexual reproduction and serves as the primary source of genetic material for the developing embryo.\\n\\nThe egg, being rich in nutrients and proteins, provides sustenance for the growing fetus during pregnancy. Moreover, it contains essential hormones that regulate the development of the embryo and the mother's body during gestation.\\n\\nDid you know that eggs come in various shapes, sizes, and colors depending on the species of animal? For instance, chicken eggs are typically oval in shape and have a smooth, thin shell, while bird eggs can range from tiny hummingbird eggs to massive ostrich eggs.\\n\\nFurthermore, eggs have been an important source of nutrition for humans for centuries. They are an excellent source of protein, vitamins, and minerals, making them a popular ingredient in many cuisines around the world. In fact, eggs are so versatile that they can be prepared in a variety of ways, from boiling and frying to scrambling and baking.\\n\\nIn conclusion, an egg is a fascinating and essential structure that has been critical to the survival and success of countless species throughout history. Its unique composition and properties make it a remarkable biological marvel!\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llama_llm | output_parser\n",
    "chain.invoke({\"input\": \"what is an egg?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model = ChatOllama(model=\"llama2:13b-chat\")\n",
    "\n",
    "text = \"What would be a good company name for a company that mekes colorful socks?\"\n",
    "messages = [HumanMessage(content=text)]\n",
    "\n",
    "type(llama_llm.invoke(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chat_model.invoke(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a good name for a company that makes colorful socks'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}\")\n",
    "prompt.format(product=\"colorful socks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant that translates English to French.'),\n",
       " HumanMessage(content='I love programming')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", template),\n",
    "        (\"human\", human_template)\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_prompt.format_messages(input_language=\"English\", output_language=\"French\", text=\"I love programming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'bye']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser = CommaSeparatedListOutputParser()\n",
    "output_parser.parse(\"hi, bye\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sure! Here is a list of animals:\\n\\nlion',\n",
       " 'tiger',\n",
       " 'bear',\n",
       " 'elephant',\n",
       " 'giraffe',\n",
       " 'zebra',\n",
       " 'monkey',\n",
       " 'kangaroo',\n",
       " 'penguin']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"Generate a list of {text}.\\n\\n{format_instructions}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(template)\n",
    "chat_prompt = chat_prompt.partial(format_instructions=output_parser.get_format_instructions())\n",
    "chain = chat_prompt | chat_model | output_parser\n",
    "chain.invoke({\"text\": \"animals\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del chat_prompt, output_parser, prompt, chat_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1131, which is longer than the specified 1000\n",
      "Created a chunk of size 1096, which is longer than the specified 1000\n",
      "Created a chunk of size 1113, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "raw_documents = UnstructuredMarkdownLoader(\"../docs/bitcoin.md\").load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "db = Chroma.from_documents(documents, OllamaEmbeddings(model=\"llama2:13b\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction\n"
     ]
    }
   ],
   "source": [
    "query = \"What is proof-of-work?\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction\n"
     ]
    }
   ],
   "source": [
    "embedding_vector = OllamaEmbeddings(model=\"llama2:13b\").embed_query(query)\n",
    "docs = db.similarity_search_by_vector(embedding_vector)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Bitcoin: A Peer-to-Peer Electronic Cash System\\n\\nauthor\\n\\n:   Satoshi Nakamoto\\n\\nemail\\n\\n:   satoshin@gmx.com\\n\\nsite\\n\\n:   http://www.bitcoin.org/', metadata={'source': '../docs/bitcoin.md'}),\n",
       " Document(page_content=\"Abstract. A purely peer-to-peer version of electronic cash would\\nallow online payments to be sent directly from one party to another\\nwithout going through a financial institution. Digital signatures\\nprovide part of the solution, but the main benefits are lost if a\\ntrusted third party is still required to prevent double-spending. We\\npropose a solution to the double-spending problem using a peer-to-peer\\nnetwork. The network timestamps transactions by hashing them into an\\nongoing chain of hash-based proof-of-work, forming a record that cannot\\nbe changed without redoing the proof-of-work. The longest chain not only\\nserves as proof of the sequence of events witnessed, but proof that it\\ncame from the largest pool of CPU power. As long as a majority of CPU\\npower is controlled by nodes that are not cooperating to attack the\\nnetwork, they\\\\'ll generate the longest chain and outpace attackers. The\\nnetwork itself requires minimal structure. Messages are broadcast on a\\nbest effort basis, and nodes can leave and rejoin the network at will,\\naccepting the longest proof-of-work chain as proof of what happened\\nwhile they were gone.\", metadata={'source': '../docs/bitcoin.md'}),\n",
       " Document(page_content='Introduction', metadata={'source': '../docs/bitcoin.md'}),\n",
       " Document(page_content='Commerce on the Internet has come to rely almost exclusively on\\nfinancial institutions serving as trusted third parties to process\\nelectronic payments. While the system works well enough for most\\ntransactions, it still suffers from the inherent weaknesses of the trust\\nbased model. Completely non-reversible transactions are not really\\npossible, since financial institutions cannot avoid mediating disputes.\\nThe cost of mediation increases transaction costs, limiting the minimum\\npractical transaction size and cutting off the possibility for small\\ncasual transactions, and there is a broader cost in the loss of ability\\nto make non-reversible payments for nonreversible services. With the\\npossibility of reversal, the need for trust spreads. Merchants must be\\nwary of their customers, hassling them for more information than they\\nwould otherwise need. A certain percentage of fraud is accepted as\\nunavoidable. These costs and payment uncertainties can be avoided in\\nperson by using physical currency, but no mechanism exists to make\\npayments over a communications channel without a trusted party', metadata={'source': '../docs/bitcoin.md'}),\n",
       " Document(page_content='What is needed is an electronic payment system based on cryptographic\\nproof instead of trust, allowing any two willing parties to transact\\ndirectly with each other without the need for a trusted third party.\\nTransactions that are computationally impractical to reverse would\\nprotect sellers from fraud, and routine escrow mechanisms could easily\\nbe implemented to protect buyers. In this paper, we propose a solution\\nto the double-spending problem using a peer-to-peer distributed\\ntimestamp server to generate computational proof of the chronological\\norder of transactions. The system is secure as long as honest nodes\\ncollectively control more CPU power than any cooperating group of\\nattacker nodes.\\n\\nTransactions', metadata={'source': '../docs/bitcoin.md'}),\n",
       " Document(page_content=\"We define an electronic coin as a chain of digital signatures. Each\\nowner transfers the coin to the next by digitally signing a hash of the\\nprevious transaction and the public key of the next owner and adding\\nthese to the end of the coin. A payee can verify the signatures to\\nverify the chain of ownership.\\n\\nThe problem of course is the payee can\\\\'t verify that one of the owners\\ndid not double-spend the coin. A common solution is to introduce a\\ntrusted central authority, or mint, that checks every transaction for\\ndouble spending. After each transaction, the coin must be returned to\\nthe mint to issue a new coin, and only coins issued directly from the\\nmint are trusted not to be double-spent. The problem with this solution\\nis that the fate of the entire money system depends on the company\\nrunning the mint, with every transaction having to go through them, just\\nlike a bank.\", metadata={'source': '../docs/bitcoin.md'}),\n",
       " Document(page_content=\"We need a way for the payee to know that the previous owners did not\\nsign any earlier transactions. For our purposes, the earliest\\ntransaction is the one that counts, so we don\\\\'t care about later\\nattempts to double-spend. The only way to confirm the absence of a\\ntransaction is to be aware of all transactions. In the mint based model,\\nthe mint was aware of all transactions and decided which arrived first.\\nTo accomplish this without a trusted party, transactions must be\\npublicly announced[^1], and we need a system for participants to agree\\non a single history of the order in which they were received. The payee\\nneeds proof that at the time of each transaction, the majority of nodes\\nagreed it was the first received.\\n\\nTimestamp Server\", metadata={'source': '../docs/bitcoin.md'}),\n",
       " Document(page_content=\"The solution we propose begins with a timestamp server. A timestamp\\nserver works by taking a hash of a block of items to be timestamped and\\nwidely publishing the hash, such as in a newspaper or Usenet\\npost[^2][^3][^4][^5]. The timestamp proves that the data must have\\nexisted at the time, obviously, in order to get into the hash. Each\\ntimestamp includes the previous timestamp in its hash, forming a chain,\\nwith each additional timestamp reinforcing the ones before it.\\n\\nProof-of-Work\\n\\nTo implement a distributed timestamp server on a peer-to-peer basis, we\\nwill need to use a proof-of-work system similar to Adam Back\\\\'s Hashcash\\n[^6], rather than newspaper or Usenet posts. The proof-of-work involves\\nscanning for a value that when hashed, such as with SHA-256, the hash\\nbegins with a number of zero bits. The average work required is\\nexponential in the number of zero bits required and can be verified by\\nexecuting a single hash.\", metadata={'source': '../docs/bitcoin.md'}),\n",
       " Document(page_content=\"For our timestamp network, we implement the proof-of-work by\\nincrementing a nonce in the block until a value is found that gives the\\nblock\\\\'s hash the required zero bits. Once the CPU effort has been\\nexpended to make it satisfy the proof-of-work, the block cannot be\\nchanged without redoing the work. As later blocks are chained after it,\\nthe work to change the block would include redoing all the blocks after\\nit\", metadata={'source': '../docs/bitcoin.md'}),\n",
       " Document(page_content='The proof-of-work also solves the problem of determining representation\\nin majority decision making. If the majority were based on\\none-IP-address-one-vote, it could be subverted by anyone able to\\nallocate many IPs. Proof-of-work is essentially one-CPU-one-vote. The\\nmajority decision is represented by the longest chain, which has the\\ngreatest proof-of-work effort invested in it. If a majority of CPU power\\nis controlled by honest nodes, the honest chain will grow the fastest\\nand outpace any competing chains. To modify a past block, an attacker\\nwould have to redo the proof-of-work of the block and all blocks after\\nit and then catch up with and surpass the work of the honest nodes. We\\nwill show later that the probability of a slower attacker catching up\\ndiminishes exponentially as subsequent blocks are added.', metadata={'source': '../docs/bitcoin.md'}),\n",
       " Document(page_content=\"To compensate for increasing hardware speed and varying interest in\\nrunning nodes over time, the proof-of-work difficulty is determined by a\\nmoving average targeting an average number of blocks per hour. If\\nthey\\\\'re generated too fast, the difficulty increases.\\n\\nNetwork\\n\\nThe steps to run the network are as follows:\\n\\nNew transactions are broadcast to all nodes.\\n\\nEach node collects new transactions into a block.\\n\\nEach node works on finding a difficult proof-of-work for its block.\\n\\nWhen a node finds a proof-of-work, it broadcasts the block to all\\n    nodes.\\n\\nNodes accept the block only if all transactions in it are valid and\\n    not already spent.\\n\\nNodes express their acceptance of the block by working on creating\\n    the next block in the chain, using the hash of the accepted block as\\n    the previous hash.\", metadata={'source': '../docs/bitcoin.md'}),\n",
       " Document(page_content='Nodes always consider the longest chain to be the correct one and will\\nkeep working on extending it. If two nodes broadcast different versions\\nof the next block simultaneously, some nodes may receive one or the\\nother first. In that case, they work on the first one they received, but\\nsave the other branch in case it becomes longer. The tie will be broken\\nwhen the next proof-of-work is found and one branch becomes longer; the\\nnodes that were working on the other branch will then switch to the\\nlonger one.\\n\\nNew transaction broadcasts do not necessarily need to reach all nodes.\\nAs long as they reach many nodes, they will get into a block before\\nlong. Block broadcasts are also tolerant of dropped messages. If a node\\ndoes not receive a block, it will request it when it receives the next\\nblock and realizes it missed one.\\n\\nIncentive', metadata={'source': '../docs/bitcoin.md'}),\n",
       " Document(page_content='By convention, the first transaction in a block is a special transaction\\nthat starts a new coin owned by the creator of the block. This adds an\\nincentive for nodes to support the network, and provides a way to\\ninitially distribute coins into circulation, since there is no central\\nauthority to issue them. The steady addition of a constant of amount of\\nnew coins is analogous to gold miners expending resources to add gold to\\ncirculation. In our case, it is CPU time and electricity that is\\nexpended.\\n\\nThe incentive can also be funded with transaction fees. If the output\\nvalue of a transaction is less than its input value, the difference is a\\ntransaction fee that is added to the incentive value of the block\\ncontaining the transaction. Once a predetermined number of coins have\\nentered circulation, the incentive can transition entirely to\\ntransaction fees and be completely inflation free.', metadata={'source': '../docs/bitcoin.md'}),\n",
       " Document(page_content=\"The incentive may help encourage nodes to stay honest. If a greedy\\nattacker is able to assemble more CPU power than all the honest nodes,\\nhe would have to choose between using it to defraud people by stealing\\nback his payments, or using it to generate new coins. He ought to find\\nit more profitable to play by the rules, such rules that favour him with\\nmore new coins than everyone else combined, than to undermine the system\\nand the validity of his own wealth.\\n\\nReclaiming Disk Space\\n\\nOnce the latest transaction in a coin is buried under enough blocks, the\\nspent transactions before it can be discarded to save disk space. To\\nfacilitate this without breaking the block\\\\'s hash, transactions are\\nhashed in a Merkle Tree[^7][^8][^9], with only the root included in the\\nblock\\\\'s hash. Old blocks can then be compacted by stubbing off branches\\nof the tree. The interior hashes do not need to be stored.\", metadata={'source': '../docs/bitcoin.md'}),\n",
       " Document(page_content=\"A block header with no transactions would be about 80 bytes. If we\\nsuppose blocks are generated every 10 minutes, 80 bytes * 6 * 24 *\\n365 = 4.2MB per year. With computer systems typically selling with 2GB\\nof RAM as of 2008, and Moore\\\\'s Law predicting current growth of 1.2GB\\nper year, storage should not be a problem even if the block headers must\\nbe kept in memory.\\n\\nSimplified Payment Verification\\n\\nIt is possible to verify payments without running a full network node. A\\nuser only needs to keep a copy of the block headers of the longest\\nproof-of-work chain, which he can get by querying network nodes until\\nhe\\\\'s convinced he has the longest chain, and obtain the Merkle branch\\nlinking the transaction to the block it\\\\'s timestamped in. He can\\\\'t\\ncheck the transaction for himself, but by linking it to a place in the\\nchain, he can see that a network node has accepted it, and blocks added\\nafter it further confirm the network has accepted it.\", metadata={'source': '../docs/bitcoin.md'}),\n",
       " Document(page_content=\"As such, the verification is reliable as long as honest nodes control\\nthe network, but is more vulnerable if the network is overpowered by an\\nattacker. While network nodes can verify transactions for themselves,\\nthe simplified method can be fooled by an attacker\\\\'s fabricated\\ntransactions for as long as the attacker can continue to overpower the\\nnetwork. One strategy to protect against this would be to accept alerts\\nfrom network nodes when they detect an invalid block, prompting the\\nuser\\\\'s software to download the full block and alerted transactions to\\nconfirm the inconsistency. Businesses that receive frequent payments\\nwill probably still want to run their own nodes for more independent\\nsecurity and quicker verification.\\n\\nCombining and Splitting Value\", metadata={'source': '../docs/bitcoin.md'}),\n",
       " Document(page_content=\"Although it would be possible to handle coins individually, it would be\\nunwieldy to make a separate transaction for every cent in a transfer. To\\nallow value to be split and combined, transactions contain multiple\\ninputs and outputs. Normally there will be either a single input from a\\nlarger previous transaction or multiple inputs combining smaller\\namounts, and at most two outputs: one for the payment, and one returning\\nthe change, if any, back to the sender.\\n\\nIt should be noted that fan-out, where a transaction depends on several\\ntransactions, and those transactions depend on many more, is not a\\nproblem here. There is never the need to extract a complete standalone\\ncopy of a transaction\\\\'s history.\\n\\nPrivacy\", metadata={'source': '../docs/bitcoin.md'}),\n",
       " Document(page_content='The traditional banking model achieves a level of privacy by limiting\\naccess to information to the parties involved and the trusted third\\nparty. The necessity to announce all transactions publicly precludes\\nthis method, but privacy can still be maintained by breaking the flow of\\ninformation in another place: by keeping public keys anonymous. The\\npublic can see that someone is sending an amount to someone else, but\\nwithout information linking the transaction to anyone. This is similar\\nto the level of information released by stock exchanges, where the time\\nand size of individual trades, the \\\\\"tape\\\\\", is made public, but without\\ntelling who the parties were.', metadata={'source': '../docs/bitcoin.md'}),\n",
       " Document(page_content='As an additional firewall, a new key pair should be used for each\\ntransaction to keep them from being linked to a common owner. Some\\nlinking is still unavoidable with multi-input transactions, which\\nnecessarily reveal that their inputs were owned by the same owner. The\\nrisk is that if the owner of a key is revealed, linking could reveal\\nother transactions that belonged to the same owner.\\n\\nCalculations\\n\\nWe consider the scenario of an attacker trying to generate an alternate\\nchain faster than the honest chain. Even if this is accomplished, it\\ndoes not throw the system open to arbitrary changes, such as creating\\nvalue out of thin air or taking money that never belonged to the\\nattacker. Nodes are not going to accept an invalid transaction as\\npayment, and honest nodes will never accept a block containing them. An\\nattacker can only try to change one of his own transactions to take back\\nmoney he recently spent.', metadata={'source': '../docs/bitcoin.md'}),\n",
       " Document(page_content=\"The race between the honest chain and an attacker chain can be\\ncharacterized as a Binomial Random Walk. The success event is the honest\\nchain being extended by one block, increasing its lead by +1, and the\\nfailure event is the attacker\\\\'s chain being extended by one block,\\nreducing the gap by -1.\\n\\nThe probability of an attacker catching up from a given deficit is\\nanalogous to a Gambler\\\\'s Ruin problem. Suppose a gambler with unlimited\\ncredit starts at a deficit and plays potentially an infinite number of\\ntrials to try to reach breakeven. We can calculate the probability he\\never reaches breakeven, or that an attacker ever catches up with the\\nhonest chain, as follows[^10]:\\n\\n| p = probability an honest node finds the next block\\n| q = probability the attacker finds the next block\\n| qz = probability the attacker will ever catch up from z blocks behind\", metadata={'source': '../docs/bitcoin.md'}),\n",
       " Document(page_content=\"$$\\\\begin{aligned}\\nq_z = \\n\\\\begin{cases}\\n1               & \\\\text{if } p \\\\leqslant q\\\\\\n\\\\left(q/p\\\\right)^z & \\\\text{if } p > q\\n\\\\end{cases}\\n\\\\end{aligned}$$\\n\\nGiven our assumption that p > q, the probability drops exponentially as\\nthe number of blocks the attacker has to catch up with increases. With\\nthe odds against him, if he doesn\\\\'t make a lucky lunge forward early\\non, his chances become vanishingly small as he falls further behind.\\n\\nWe now consider how long the recipient of a new transaction needs to\\nwait before being sufficiently certain the sender can\\\\'t change the\\ntransaction. We assume the sender is an attacker who wants to make the\\nrecipient believe he paid him for a while, then switch it to pay back to\\nhimself after some time has passed. The receiver will be alerted when\\nthat happens, but the sender hopes it will be too late\", metadata={'source': '../docs/bitcoin.md'}),\n",
       " Document(page_content=\"The receiver generates a new key pair and gives the public key to the\\nsender shortly before signing. This prevents the sender from preparing a\\nchain of blocks ahead of time by working on it continuously until he is\\nlucky enough to get far enough ahead, then executing the transaction at\\nthat moment. Once the transaction is sent, the dishonest sender starts\\nworking in secret on a parallel chain containing an alternate version of\\nhis transaction.\\n\\nThe recipient waits until the transaction has been added to a block and\\nz blocks have been linked after it. He doesn\\\\'t know the exact amount of\\nprogress the attacker has made, but assuming the honest blocks took the\\naverage expected time per block, the attacker\\\\'s potential progress will\\nbe a Poisson distribution with expected value:\\n\\n$$\\\\lambda = z \\\\frac{q}{p}$$\", metadata={'source': '../docs/bitcoin.md'}),\n",
       " Document(page_content='To get the probability the attacker could still catch up now, we\\nmultiply the Poisson density for each amount of progress he could have\\nmade by the probability he could catch up from that point:\\n\\n$$\\\\begin{aligned}\\n\\\\sum _{k=0}^\\\\infty \\\\frac{\\\\lambda ^k e^{-\\\\lambda}}{k!} \\\\cdot \\n\\\\begin{cases}\\n\\\\left(q/p\\\\right)^{(z-p)} & \\\\text{if } k \\\\leqslant z \\\\\\n1                     & \\\\text{if } k > z\\n\\\\end{cases}\\n\\\\end{aligned}$$\\n\\nRearranging to avoid summing the infinite tail of the distribution...\\n\\n$$1 - \\\\sum _{k=0}^z \\\\frac{\\\\lambda ^k e^{-\\\\lambda}}{k!} \\\\left(1 - \\\\left(q/p\\\\right)^{(z-k)}\\\\right)$$\\n\\nConverting to C code...\\n\\n``` {.c}\\n\\ninclude\\n\\ndouble AttackerSuccessProbability(double q, int z)\\n{\\n    double p = 1.0 - q;\\n    double lambda = z * (q / p);\\n    double sum = 1.0;\\n    int i, k;\\n    for (k = 0; k <= z; k++)\\n    {\\n        double poisson = exp(-lambda);\\n        for (i = 1; i <= k; i++)\\n            poisson *= lambda / i;\\n        sum -= poisson * (1 - pow(q / p, z - k));\\n    }\\n    return sum;\\n}\\n```', metadata={'source': '../docs/bitcoin.md'}),\n",
       " Document(page_content='Running some results, we can see the probability drop off exponentially\\nwith z.\\n\\nSolving for P less than 0.1%...\\n\\nConclusion', metadata={'source': '../docs/bitcoin.md'}),\n",
       " Document(page_content='We have proposed a system for electronic transactions without relying on\\ntrust. We started with the usual framework of coins made from digital\\nsignatures, which provides strong control of ownership, but is\\nincomplete without a way to prevent double-spending. To solve this, we\\nproposed a peer-to-peer network using proof-of-work to record a public\\nhistory of transactions that quickly becomes computationally impractical\\nfor an attacker to change if honest nodes control a majority of CPU\\npower. The network is robust in its unstructured simplicity. Nodes work\\nall at once with little coordination. They do not need to be identified,\\nsince messages are not routed to any particular place and only need to\\nbe delivered on a best effort basis. Nodes can leave and rejoin the\\nnetwork at will, accepting the proof-of-work chain as proof of what\\nhappened while they were gone. They vote with their CPU power,\\nexpressing their acceptance of valid blocks by working on extending them\\nand rejecting invalid blocks by refusing to work on them. Any needed\\nrules and incentives can be enforced with this consensus mechanism.', metadata={'source': '../docs/bitcoin.md'}),\n",
       " Document(page_content='References\\n\\n[^1]: W.  Dai, \\\\\"b-money,\\\\\" http://www.weidai.com/bmoney.txt, 1998.\\n\\n[^2]: H.  Massias, X.S. Avila, and J.-J. Quisquater, \\\\\"Design of a\\n        secure timestamping service with minimal trust requirements,\\\\\"\\n        In 20th Symposium on Information Theory in the Benelux,\\n        May 1999.\\n\\n[^3]: S.  Haber, W.S. Stornetta, \\\\\"How to time-stamp a digital\\n        document,\\\\\" In Journal of Cryptology, vol 3, no 2, pages\\n        99-111, 1991.\\n\\n[^4]: D.  Bayer, S. Haber, W.S. Stornetta, \\\\\"Improving the efficiency\\n        and reliability of digital time-stamping,\\\\\" In Sequences II:\\n        Methods in Communication, Security and Computer Science, pages\\n        329-334, 1993.\\n\\n[^5]: S.  Haber, W.S. Stornetta, \\\\\"Secure names for bit-strings,\\\\\" In\\n        Proceedings of the 4th ACM Conference on Computer and\\n        Communications Security, pages 28-35, April 1997.', metadata={'source': '../docs/bitcoin.md'}),\n",
       " Document(page_content='[^6]: A.  Back, \\\\\"Hashcash - a denial of service counter-measure,\\\\\"\\n        http://www.hashcash.org/papers/hashcash.pdf, 2002.\\n\\n[^7]: R.C. Merkle, \\\\\"Protocols for public key cryptosystems,\\\\\" In Proc.\\n    1980 Symposium on Security and Privacy, IEEE Computer Society, pages\\n    122-133, April 1980.\\n\\n[^8]: H.  Massias, X.S. Avila, and J.-J. Quisquater, \\\\\"Design of a\\n        secure timestamping service with minimal trust requirements,\\\\\"\\n        In 20th Symposium on Information Theory in the Benelux,\\n        May 1999.\\n\\n[^9]: S.  Haber, W.S. Stornetta, \\\\\"Secure names for bit-strings,\\\\\" In\\n        Proceedings of the 4th ACM Conference on Computer and\\n        Communications Security, pages 28-35, April 1997.\\n\\n[^10]: W.  Feller, \\\\\"An introduction to probability theory and its\\n        applications,\\\\\" 1957.', metadata={'source': '../docs/bitcoin.md'})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"llama2:13b\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    Question: {input}\"\"\"\n",
    ")\n",
    "document_chain = create_stuff_documents_chain(llama_llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Based on the provided context, LangSmith can help with testing in the following ways:\n",
      "\n",
      "1. Rapid experimentation: LangSmith allows for quick experimentation with different prompts, models, and retrieval strategies, making it easier to test and iterate on LLM applications.\n",
      "2. Debugging: LangSmith provides clear visibility and debugging information at each step of an LLM sequence, making it easier to identify and root-cause issues.\n",
      "3. Testing: LangSmith allows developers to create datasets, which are collections of inputs and reference outputs, and use these to run tests on their LLM applications.\n",
      "4. Comparison view: LangSmith provides a comparison view for test runs to track and diagnose regressions in test scores across multiple revisions of the application.\n",
      "5. Beta testing: LangSmith supports beta testing, which allows developers to collect more data on how their LLM applications are performing in real-world scenarios.\n",
      "6. Annotating traces: LangSmith also supports sending runs to annotation queues, which allow annotators to closely inspect interesting traces and annotate them with respect to different criteria.\n",
      "7. Adding runs to a dataset: LangSmith enables developers to add runs as examples to datasets, expanding test coverage on real-world scenarios.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\n",
    "    \"input\": \"how can langsmith help with the testing?\"\n",
    "})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversation Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Given the above converstaion, generate a search query to look up to get information relevant to the conversation\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "retrieval_chain = create_history_aware_retriever(llama_llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='applications are multi-turn, meaning that they involve a series of interactions between the user and the application. LangSmith provides a threads view that groups traces from a single conversation together, making it easier to track the performance of and annotate your application across multiple turns.Was this page helpful?PreviousQuick StartNextOverviewPrototypingBeta TestingProductionCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2024 LangChain, Inc.', metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.', 'language': 'en'}),\n",
       " Document(page_content='LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.', 'language': 'en'}),\n",
       " Document(page_content='Skip to main contentLangSmith API DocsSearchGo to AppQuick StartUser GuideTracingEvaluationProduction Monitoring & AutomationsPrompt HubProxyPricingSelf-HostingCookbookUser GuideOn this pageLangSmith User GuideLangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.Prototyping‚ÄãPrototyping LLM applications often involves quick experimentation between prompts, model types, retrieval strategy and other parameters.\\nThe ability to rapidly understand how the model is performing ‚Äî and debug where it is failing ‚Äî is incredibly important for this phase.Debugging‚ÄãWhen developing new LLM applications, we suggest having LangSmith tracing enabled by default.\\nOftentimes, it isn‚Äôt necessary to look at every single trace. However, when things go wrong (an unexpected end result, infinite agent loop, slower than expected execution, higher than expected token usage), it‚Äôs extremely helpful to debug by looking through the application traces. LangSmith gives clear visibility and debugging information at each step of an LLM sequence, making it much easier to identify and root-cause issues.\\nWe provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set‚ÄãWhile many developers still ship an initial version of their application based on ‚Äúvibe checks‚Äù, we‚Äôve seen an increasing number of engineering teams start to adopt a more test driven approach. LangSmith allows developers to create datasets, which are collections of inputs and reference outputs, and use these to run tests on their LLM applications.\\nThese test cases can be uploaded in bulk, created on the fly, or exported from application traces. LangSmith also makes it easy to run custom evaluations (both LLM and heuristic based) to score test results.Comparison View‚ÄãWhen prototyping different versions of your applications and making changes, it‚Äôs important to see whether or not you‚Äôve regressed with respect to your initial test cases.\\nOftentimes, changes in the prompt, retrieval strategy, or model choice can have huge implications in responses produced by your application.\\nIn order to get a sense for which variant is performing better, it‚Äôs useful to be able to view results for different configurations on the same datapoints side-by-side. We‚Äôve invested heavily in a user-friendly comparison view for test runs to track and diagnose regressions in test scores across multiple revisions of your application.Playground‚ÄãLangSmith provides a playground environment for rapid iteration and experimentation.\\nThis allows you to quickly test out different prompts and models. You can open the playground from any prompt or model run in your trace.', metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.', 'language': 'en'}),\n",
       " Document(page_content=\"Every playground run is logged in the system and can be used to create test cases or compare with other runs.Beta Testing‚ÄãBeta testing allows developers to collect more data on how their LLM applications are performing in real-world scenarios. In this phase, it‚Äôs important to develop an understanding for the types of inputs the app is performing well or poorly on and how exactly it‚Äôs breaking down in those cases. Both feedback collection and run annotation are critical for this workflow. This will help in curation of test cases that can help track regressions/improvements and development of automatic evaluations.Capturing Feedback‚ÄãWhen launching your application to an initial set of users, it‚Äôs important to gather human feedback on the responses it‚Äôs producing. This helps draw attention to the most interesting runs and highlight edge cases that are causing problematic responses. LangSmith allows you to attach feedback scores to logged traces (oftentimes, this is hooked up to a feedback button in your app), then filter on traces that have a specific feedback tag and score. A common workflow is to filter on traces that receive a poor user feedback score, then drill down into problematic points using the detailed trace view.Annotating Traces‚ÄãLangSmith also supports sending runs to annotation queues, which allow annotators to closely inspect interesting traces and annotate them with respect to different criteria. Annotators can be PMs, engineers, or even subject matter experts. This allows users to catch regressions across important evaluation criteria.Adding Runs to a Dataset‚ÄãAs your application progresses through the beta testing phase, it's essential to continue collecting data to refine and improve its performance. LangSmith enables you to add runs as examples to datasets (from both the project page and within an annotation queue), expanding your test coverage on real-world scenarios. This is a key benefit in having your logging system and your evaluation/testing system in the same platform.Production‚ÄãClosely inspecting key data points, growing benchmarking datasets, annotating traces, and drilling down into important data in trace view are workflows you‚Äôll also want to do once your app hits production.However, especially at the production stage, it‚Äôs crucial to get a high-level overview of application performance with respect to latency, cost, and feedback scores. This ensures that it's delivering desirable results at scale.Online evaluations and automations allow you to process and score production traces in near real-time.Additionally, threads provide a seamless way to group traces from a single conversation, making it easier to track the performance of your application across multiple turns.Monitoring and A/B Testing‚ÄãLangSmith provides monitoring charts that allow you to track key metrics over time. You can expand to view metrics for a given period and drill down into a specific data point to get a trace table for that time period ‚Äî this is especially handy for debugging production issues.LangSmith also allows for tag and metadata grouping, which allows users to mark different versions of their applications with different identifiers and view how they are performing side-by-side within each chart. This is helpful for A/B testing changes in prompt, model, or retrieval strategy.Automations‚ÄãAutomations are a powerful feature in LangSmith that allow you to perform actions on traces in near real-time. This can be used to automatically score traces, send them to annotation queues, or send them to datasets.To define an automation, simply provide a filter condition, a sampling rate, and an action to perform. Automations are particularly helpful for processing traces at production scale.Threads‚ÄãMany LLM applications are multi-turn, meaning that they involve a series of interactions between the user and the application. LangSmith provides a threads view that groups traces from a single conversation\", metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.', 'language': 'en'})]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM application?\")]\n",
    "retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Answer the user's questions based on the below context: \\n\\n{context}\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "document_chain = create_stuff_documents_chain(llama_llm, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retrieval_chain, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Can LangSmith help test my LLM applications?'),\n",
       "  AIMessage(content='Yes!')],\n",
       " 'input': 'Tell me how',\n",
       " 'context': [Document(page_content='applications are multi-turn, meaning that they involve a series of interactions between the user and the application. LangSmith provides a threads view that groups traces from a single conversation together, making it easier to track the performance of and annotate your application across multiple turns.Was this page helpful?PreviousQuick StartNextOverviewPrototypingBeta TestingProductionCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2024 LangChain, Inc.', metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.', 'language': 'en'}),\n",
       "  Document(page_content='LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.', 'language': 'en'}),\n",
       "  Document(page_content='Skip to main contentLangSmith API DocsSearchGo to AppQuick StartUser GuideTracingEvaluationProduction Monitoring & AutomationsPrompt HubProxyPricingSelf-HostingCookbookUser GuideOn this pageLangSmith User GuideLangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.Prototyping‚ÄãPrototyping LLM applications often involves quick experimentation between prompts, model types, retrieval strategy and other parameters.\\nThe ability to rapidly understand how the model is performing ‚Äî and debug where it is failing ‚Äî is incredibly important for this phase.Debugging‚ÄãWhen developing new LLM applications, we suggest having LangSmith tracing enabled by default.\\nOftentimes, it isn‚Äôt necessary to look at every single trace. However, when things go wrong (an unexpected end result, infinite agent loop, slower than expected execution, higher than expected token usage), it‚Äôs extremely helpful to debug by looking through the application traces. LangSmith gives clear visibility and debugging information at each step of an LLM sequence, making it much easier to identify and root-cause issues.\\nWe provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set‚ÄãWhile many developers still ship an initial version of their application based on ‚Äúvibe checks‚Äù, we‚Äôve seen an increasing number of engineering teams start to adopt a more test driven approach. LangSmith allows developers to create datasets, which are collections of inputs and reference outputs, and use these to run tests on their LLM applications.\\nThese test cases can be uploaded in bulk, created on the fly, or exported from application traces. LangSmith also makes it easy to run custom evaluations (both LLM and heuristic based) to score test results.Comparison View‚ÄãWhen prototyping different versions of your applications and making changes, it‚Äôs important to see whether or not you‚Äôve regressed with respect to your initial test cases.\\nOftentimes, changes in the prompt, retrieval strategy, or model choice can have huge implications in responses produced by your application.\\nIn order to get a sense for which variant is performing better, it‚Äôs useful to be able to view results for different configurations on the same datapoints side-by-side. We‚Äôve invested heavily in a user-friendly comparison view for test runs to track and diagnose regressions in test scores across multiple revisions of your application.Playground‚ÄãLangSmith provides a playground environment for rapid iteration and experimentation.\\nThis allows you to quickly test out different prompts and models. You can open the playground from any prompt or model run in your trace.', metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.', 'language': 'en'}),\n",
       "  Document(page_content=\"Every playground run is logged in the system and can be used to create test cases or compare with other runs.Beta Testing‚ÄãBeta testing allows developers to collect more data on how their LLM applications are performing in real-world scenarios. In this phase, it‚Äôs important to develop an understanding for the types of inputs the app is performing well or poorly on and how exactly it‚Äôs breaking down in those cases. Both feedback collection and run annotation are critical for this workflow. This will help in curation of test cases that can help track regressions/improvements and development of automatic evaluations.Capturing Feedback‚ÄãWhen launching your application to an initial set of users, it‚Äôs important to gather human feedback on the responses it‚Äôs producing. This helps draw attention to the most interesting runs and highlight edge cases that are causing problematic responses. LangSmith allows you to attach feedback scores to logged traces (oftentimes, this is hooked up to a feedback button in your app), then filter on traces that have a specific feedback tag and score. A common workflow is to filter on traces that receive a poor user feedback score, then drill down into problematic points using the detailed trace view.Annotating Traces‚ÄãLangSmith also supports sending runs to annotation queues, which allow annotators to closely inspect interesting traces and annotate them with respect to different criteria. Annotators can be PMs, engineers, or even subject matter experts. This allows users to catch regressions across important evaluation criteria.Adding Runs to a Dataset‚ÄãAs your application progresses through the beta testing phase, it's essential to continue collecting data to refine and improve its performance. LangSmith enables you to add runs as examples to datasets (from both the project page and within an annotation queue), expanding your test coverage on real-world scenarios. This is a key benefit in having your logging system and your evaluation/testing system in the same platform.Production‚ÄãClosely inspecting key data points, growing benchmarking datasets, annotating traces, and drilling down into important data in trace view are workflows you‚Äôll also want to do once your app hits production.However, especially at the production stage, it‚Äôs crucial to get a high-level overview of application performance with respect to latency, cost, and feedback scores. This ensures that it's delivering desirable results at scale.Online evaluations and automations allow you to process and score production traces in near real-time.Additionally, threads provide a seamless way to group traces from a single conversation, making it easier to track the performance of your application across multiple turns.Monitoring and A/B Testing‚ÄãLangSmith provides monitoring charts that allow you to track key metrics over time. You can expand to view metrics for a given period and drill down into a specific data point to get a trace table for that time period ‚Äî this is especially handy for debugging production issues.LangSmith also allows for tag and metadata grouping, which allows users to mark different versions of their applications with different identifiers and view how they are performing side-by-side within each chart. This is helpful for A/B testing changes in prompt, model, or retrieval strategy.Automations‚ÄãAutomations are a powerful feature in LangSmith that allow you to perform actions on traces in near real-time. This can be used to automatically score traces, send them to annotation queues, or send them to datasets.To define an automation, simply provide a filter condition, a sampling rate, and an action to perform. Automations are particularly helpful for processing traces at production scale.Threads‚ÄãMany LLM applications are multi-turn, meaning that they involve a series of interactions between the user and the application. LangSmith provides a threads view that groups traces from a single conversation\", metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we‚Äôll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they‚Äôre just starting their journey.', 'language': 'en'})],\n",
       " 'answer': \"\\nLangSmith is a platform for developing, monitoring, and testing LLM (LLM) applications. It provides a variety of features and functionalities to support the entire development lifecycle, from prototyping to production. Here are some ways LangSmith can help test your LLM applications:\\n\\n1. Rapid experimentation: LangSmith allows for quick experimentation with different prompts, models, retrieval strategies, and other parameters. This is especially useful during the prototyping phase, where developers need to quickly test and iterate on different ideas.\\n2. Debugging: When developing new LLM applications, it's important to have a clear understanding of how the model is performing. LangSmith provides detailed debugging information at each step of an LLM sequence, making it easier to identify and root-cause issues.\\n3. Testing: LangSmith allows developers to create datasets of inputs and reference outputs, and use these to run tests on their LLM applications. This helps ensure that the application is performing as expected and can be used to track regressions and improvements over time.\\n4. Comparison view: When testing different versions of an application, it's important to compare the results side-by-side. LangSmith provides a user-friendly comparison view for test runs, allowing developers to easily see which variant is performing better.\\n5. Playground: LangSmith provides a playground environment for rapid iteration and experimentation. This allows developers to quickly try out new ideas and see how they perform.\\n6. Annotating traces: LangSmith supports sending runs to annotation queues, which allow annotators to closely inspect interesting traces and annotate them with respect to different criteria.\\n7. Adding runs to a dataset: As an application progresses through the beta testing phase, it's essential to continue collecting data to refine and improve its performance. LangSmith enables you to add runs as examples to datasets from both the project page and within an annotation queue.\\n8. Monitoring and A/B testing: LangSmith provides monitoring charts that allow you to track key metrics over time. This is especially handy for debugging production issues. It also allows users to mark different versions of their applications with different identifiers and view how they are performing side-by-side within each chart, which is helpful for A/B testing changes in prompt, model, or retrieval strategy.\\n9. Automations: LangSmith allows you to perform actions on traces in near real-time. This can be used to automatically score traces, send them to annotation queues, or send them to datasets.\\n10. Threads: Many LLM applications are multi-turn, meaning that they involve a series of interactions between the user and the application. LangSmith provides a threads view that groups traces from a single conversation, making it easier to track the performance of your application across multiple turns.\"}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = [\n",
    "    HumanMessage(content=\"Can LangSmith help test my LLM applications?\"),\n",
    "    AIMessage(content=\"Yes!\"),\n",
    "]\n",
    "retrieval_chain.invoke({\"chat_history\": chat_history, \"input\": \"Tell me how\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith_search\",\n",
    "    \"Search for information about LangSmith. For any question about LangSmith, you must use this tool!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = TavilySearchResults()\n",
    "tools = [retriever_tool, search]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
